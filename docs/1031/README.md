# AI Safety

ğŸ“º <https://www.youtube.com/watch?v=agEPmYdbQLs>

**Full Course by Safe.AI Founder on Machine Learning & Ethics (Center for AI Safety)**

ML systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In this course weâ€™ll discuss how researchers can shape the process that will lead to strong AI systems and steer that process in a safer direction. Weâ€™ll cover various technical topics to reduce existential risks (X-Risks) from strong AI, namely withstanding hazards (â€œRobustnessâ€), identifying hazards (â€œMonitoringâ€), reducing inherent ML system hazards (â€œAlignmentâ€), and reducing systemic hazards (â€œSystemic Safetyâ€). At the end, we will zoom out and discuss additional abstract existential hazards and discuss how to increase safety without unintended side effects.


* See <https://course.mlsafety.org> for more.
* Website for the Center for AI Safety: <https://www.safe.ai/> 
* Safe.ai newsletter: <https://newsletter.safe.ai/>

* Course Developed By: **Dr. Dan Hendrycks** 

---


* Deep Learning Review
* Risk Decomposition
* Accident Models
* Black Swans
* Adversarial Robustness
* Black Swan Robustness
* Anomaly Detection
* Interpretable Uncertainty
* Transparency
* Trojans
* Detecting Emergent Behavior
* Honest Models
* Machine Ethics
* ML for Improved Decision-Making
* ML for Cyberdefense
* Cooperative AI
* X-Risk Overview
* Possible Existential Hazards
* AI and Evolution
* Safety-Capabilities Balance
* Review and Conclusion


Documentation By: **Raymond C. TURNER**

Last Updated: 1 Day ago