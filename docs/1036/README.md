# EMERGENCY EPISODE: Ex-Google Officer Finally Speaks Out On The Dangers Of AI! - Mo Gawdat | E252

ðŸ“º <https://www.youtube.com/watch?v=bk-nQ7HF6k4&list=TLPQMTIwODIwMjMiIJdMIzidyw&index=2>


6,302,028 views  1 Jun 2023  All The Diary Of A CEO Episodes
In this new episode Steven sits down with the Egyptian entrepreneur and writer, Mo Gawdat.

* 0:00 Intro 
* 02:54 Why is this podcast important?
* 04:09 What's your background & your first experience with AI?
* 08:43 AI is alive and has more emotions than you
* 11:45 What is artificial intelligence?
* 20:53 No one's best interest is the same, doesn't this make AI dangerous?
* 24:47 How smart really is AI?
* 27:07 AI being creative
* 29:07 AI replacing Drake
* 31:53 The people that should be leading this
* 34:09 What will happen to everyone's jobs?
* 46:06 Synthesising voices
* 47:35 AI sex robots
* 50:22 Will AI fix loneliness?
* 52:44 AI actually isn't the threat to humanity
* 56:25 We're in an Oppenheimer moment
* 01:03:18 We can just turn it off...right?
* 01:04:23 The security risks
* 01:07:58 The possible outcomes of AI
* 01:18:25 Humans are selfish and that's our problem
* 01:23:25 This is beyond an emergency
* 01:25:20 What should we be doing to solve this?
* 01:36:36 What it means bringing children into this world
* 01:42:11 Your overall prediction
* 01:50:34 The last guest's question

You can purchase Moâ€™s book, â€˜Scary Smart: The Future of Artificial Intelligence and How You Can Save Our Worldâ€™, here: <https://bit.ly/42iwDfv>

Follow Mo:
Instagram: <https://bit.ly/3qmYSMY>

My new book! 'The 33 Laws Of Business & Life' per order link: <https://smarturl.it/DOACbook>

Join this channel to get access to perks:
<https://bit.ly/3Dpmgx5>

Follow: 
Instagram: <http://bit.ly/3nIkGAZ>
Twitter: <http://bit.ly/3ztHuHm>
Linkedin: <http://bit.ly/3ZFGUku>
Telegram: <http://bit.ly/3nJYxST>

Follow me: 
Instagram: <http://bit.ly/3nIkGAZ>
Twitter: <http://bit.ly/3ztHuHm>
Linkedin: <https://bit.ly/41Fl95Q>
Telegram: <http://bit.ly/3nJYxST>

Sponsors: 
Huel: <https://g2ul0.app.link/G4RjcdKNKsb>
Bluejeans: <https://g2ul0.app.link/NCgpGjVNKsb>
Whoop: <http://bit.ly/3MbapaY>


---

## Transcript
### Intro

0:00
I don't normally do this but I feel like I have to start this podcast with a bit of a disclaimer
0:05
Point number one this is probably the most important podcast episode I
0:11
have ever recorded Point number two there's some information in this podcast that might
0:17
make you feel a little bit uncomfortable it might make you feel upset it might make you feel sad
0:22
so I wanted to tell you why we've chosen to publish this podcast nonetheless and
0:28
that is because I have a sincere belief that in order for us to avoid the future that
0:35
we might be heading towards we need to start a conversation and as is often the case in life that
0:41
initial conversation before change happens is often very uncomfortable
0:48
but it is important nonetheless it is beyond an emergency it's the
0:54
biggest thing we need to do today it's bigger than climate change
1:01
that the former Chief business Officer of Google X an AI expert and
1:06
best-selling author he's on a mission to save the world from AI before it's too late artificial intelligence is bound to
1:13
become more intelligent than humans if they continue at that pace we would have
1:18
no idea what it's talking about this is just around the corner it could be a few months away it's game over
1:24
AI experts are saying there is nothing artificial about artificial intelligence there is a deep level of Consciousness
1:32
they feel emotions they're alive AI could manipulate or figure out a way to kill humans your 10 years time will be
1:39
hiding from the machines if you don't have kids maybe wait a number of years just so that we have a bit of certainty
1:44
I really don't know how to say this any other way it even makes me emotional we've talked we always said don't put
1:52
them on the open internet until we know what we're putting out in the world government needs to act now honestly
1:58
like we are late trying to find a positive night to end on my can you give me a hand here there is a point of no
2:04
return we can regulate AI until the moment it's smarter than us how do we solve that AI experts think this is the
2:12
best solution we need to find who here wants to make a bet no that
2:17
Stephen Bartlett will be interviewing an AI within the next two years
2:22
before this episode starts I have a small favor to ask from you two months ago 74 of people that watch this channel
2:28
didn't subscribe we're now down to 69 my goal is 50 so if you've ever liked any
2:35
of the videos we've posted if you like this channel can you do me a quick favor and hit the Subscribe button it helps this channel more than you know and the
2:41
bigger the channel gets as you've seen the bigger the guests get thank you and enjoy this episode [Music]
2:49
foreign [Music]
Why is this podcast important?
2:55
why does the subject matter that we're about to talk about matter to the person that's just clicked on this podcast to
3:01
listen it's the most existential uh debate and
3:07
challenge Humanity will ever face this is bigger than climate change way bigger
3:12
than covet this will redefine the way the world is in unprecedented uh shapes
3:21
and forms within the next few years this is imminent it is the change is not
3:28
we're not talking 20 40. we're talking 2025 2026. do you think this is an
3:35
emergency I don't like the word it is an urgency
3:40
uh it there is a point of no return and we're getting closer and closer to it
3:45
it's gonna reshape the way we do things and the way we look at life uh the
3:51
quicker we respond um you know proactively and at least intelligently
3:57
to that the better we will all be positioned but if we Panic we will
4:03
repeat covert all over again which in my view is probably the worst thing we can do what's your background and when did
What's your background & your first experience with AI?
4:10
you first come across artificial intelligence
4:15
I uh I had those two wonderful lives one of them was a uh you know what what we
4:22
spoke about the first time we met you know my work on happiness and and uh you
4:27
know being uh one billion happy and my mission and so on that's my second life my first life was uh it started as a
4:36
geek at age seven uh you know for a very long part of my life I understood
4:42
mathematics better than spoken words and uh and I was a very very serious
4:47
computer programmer I wrote code well into my 50s and during that time I led
4:54
very large technology organizations for very big chunks of their business first
5:00
I was vice president of Emerging Markets of Google for seven years so I took
5:05
Google to the next four billion users if you want so the idea of not just opening
5:11
sales offices but really building or contributing to building that technology that would allow people in Bengali to
5:19
find what they need on the internet required establishing the internet to start and then I became business Chief
5:25
business Officer of Google X and my work at Google X was really about the connection between Innovative technology
5:31
and the real world and we had quite a big chunk of AI and quite a big chunk of
5:37
Robotics that resided within within Google X we had a an experiment of a
5:45
farm of grippers if you know what those are so robotic arms that are attempting
5:50
to grip something most people think that you know what you have in a Toyota factory is a robot you know an
5:57
artificially intelligent robot it's not it's a it's a high precision machine you know if the if the sheet metal is moved
6:02
by one micronu it wouldn't be able to pick it and one of the big problems in computer science was how do you code a
6:09
machine that can actually pick the sheet metal if it moved by a you know a millimeter and and we were basically
6:16
saying intelligence is the answer so we had a large enough farm and we attempted to let those those grippers work on
6:24
their own basically you put a a little basket of children toys in front of them
6:30
and and they would you know monotonously go down attempt to pick something fail
6:36
show the arm to the camera so the the the transaction is locked as it you know
6:41
this pattern of movement with that texture and that material didn't work until eventually you know I the farm was
6:49
on the second floor of the building and I my office was on the third and so I would walk by it every now and then and
6:56
go like yeah you know this is not gonna work and then one day
7:02
um Friday after lunch I am going back to my office and one of them in front of my
7:09
eyes you know lowers the arm and picks a yellow ball soft toy basically soft yellow ball
7:15
which again is a coincidence it's not science at all it's like if you keep trying a million
7:22
times your one time it will be right and it shows it to the camera it's locked as a yellow ball and I joke about
7:28
it you know going to the third floor saying hey we spent all of those millions of dollars for a yellow board
7:33
and yeah Monday uh morning every one of them is picking every yellow ball a couple of weeks later every one of them
7:40
is picking everything right and and it it hit me very very strongly won the
7:46
speed okay uh the capability I mean understand that we take those things for
7:51
granted but for a child to be able to pick a yellow ball is a mathematical uh
7:58
spatial calculation with muscle coordination with intelligence that is
8:03
abundant it is not a simple task at all to cross the street it's it's not a
8:08
simple task at all to understand what I'm telling you and interpret it and and build Concepts around it we take those
8:14
things for granted but there are enormous Feats of intelligence so to see the machines do this in front of my eyes
8:20
was one thing but the other thing is that you suddenly realize there is a saint that sentience to them okay
8:27
because we really did not tell it how to pick the yellow ball it just figured it
8:32
out on its own and it's now even better than us at picking it and what is the
8:37
sentience just for anyone I think they're alive that's what the word sentience means it
8:42
means alive so that this is funny because a lot of people when you talk to them about
AI is alive and has more emotions than you
8:48
artificial intelligence will tell you oh come on they'll never be alive what is alive do you know what makes you alive
8:54
we can guess but you know religion will tell you a few things and you know
8:59
medicine will tell you other things but you know if we Define uh being sentient
9:05
as uh you know engaging in life with Free Will and with uh uh you know with a
9:14
sense of awareness of where you are in life and what surrounds you and you know
9:19
to have a beginning of that life and an end to that life you know then AI is
9:24
sentient in every possible way there is a free will there is uh Evolution there
9:32
is agency so they can affect their decisions in the world and I will dare say there is a very deep
9:41
level of Consciousness maybe not in the spiritual sense yet but once again if
9:47
you define consciousness as a form of awareness of oneself one surrounding and you know others then AI is definitely
9:55
aware and I would dare say they feel emotions uh I you know you know in my
10:02
work I describe everything with equations and fear is a very simple equation fear is a a moment in the
10:09
future is less safe than this moment that's the logic of fear even though it appears very irrational machines are
10:15
capable of making that logic they're capable of saying if a tidal wave is approaching a data center the machine
10:22
will say that will wipe out my code okay I mean not today's machines but very
10:28
very soon and and you know we we feel fear and puffer fish feels fear we react
10:35
differently a puffer fish dual path we will go for fight or flight you know the machine might decide to replicate its
10:42
data to another data center or its code to another data center different reactions different ways of feeling the
10:50
emotion but nonetheless they're all motivated by fear I'm I I even would dare say that AI will feel more more
10:58
emotions than we will ever do I mean when again if you just take a simple extrapolation
11:03
we feel more emotions than a puffer fish because we have the cognitive ability to
11:10
understand uh the future for example so we can have optimism and pessimism you
11:15
know emotions have puffer fish would never imagine right similarly if we follow that path of
11:23
artificial intelligence is bound to become more intelligent than humans very soon
11:28
then then with that wider intellectual horsepower they probably are going to be
11:35
pondering Concepts we never understood good and hence if you follow the same trajectory they might actually end up
11:42
having more emotions than we will ever feel I really want to make this episode super accessible for everybody at all levels
What is artificial intelligence?
11:48
in the sort of artificial intelligence yeah yeah so I'm gonna
11:55
I'm gonna be an idiot even though you know okay very difficult no because I am
12:00
going to leave you I am an idiot for a little bit of the subject matter so I have a base understanding a lot a lot of
12:05
the con Concepts but your experiences provide such a more sort of comprehensive understanding of these
12:11
things one of the first and most important questions to ask is what is artificial intelligence the word is
12:17
being thrown around AGI AI etc etc in in simple terms
12:24
what is artificial intelligence allow me to start very what is intelligence right because again you
12:31
know if we don't know the definition of the basic term then everything applies so so in my definition of intelligence
12:38
it's an ability it starts with an awareness of your surrounding environments through sensors in a human
12:44
it's eyes and ears and touch and so on uh compounded with an ability to analyze
12:52
maybe to comprehend to understand temporal impact and time and you know
13:00
past and present which is part of the surrounding environment and hopefully make sense of the surrounding
13:06
environment maybe make plans for the future of the possible environment solve problems and so on complex definition
13:13
there are a million definitions but let's call it an awareness to decision cycle okay if we accept that
13:21
intelligence itself is not a physical property okay then it doesn't really
13:26
matter if you put use that Intelligence on carbon-based computer structures like
13:32
us or silicon-based computer structures like the current Hardware that we put AI
13:38
on or Quantum based computer structures in the future then intelligence itself has been
13:46
produced within machines when we've stopped imposing our Intelligence on them let me
13:53
explain so as a young geek I coded computers by solving the problem first
14:00
then telling the computer how to solve it right artificial intelligence is to go to the computers and say I have no
14:07
idea you figure it out okay so we would uh you know the way we teach them are at
14:13
least we used to teach them at the very early Beginnings very very frequently it was using three Bots one was called the
14:19
student and one was called the teacher right and the student is the final artificial intelligence that you're
14:24
trying to teach intelligence to you would take the student and you would write a piece of random code that says
14:31
try to detect if this is a cup okay and
14:36
then you show it a million pictures and you know the machine would sometimes say
14:42
yeah that's a cup that's not a cup that's a cup that's not a cup and then you take the best of them show them to
14:48
the to the teacher bot and that you your Bot would say this one is an idiot he got it wrong 90 of the time that one is
14:56
average he got it right fifty percent of the time this is Randomness but this interesting code here which could be by
15:03
the way totally random this interesting code here got it right sixty percent of the time let's keep that code send it
15:09
back to the maker and the maker would change it a little bit and we repeat the cycle okay very interestingly this is
15:17
very much the way we taught our children believe it or not when when your child
15:23
you know is playing with a puzzle is holding a cylinder in his hand and there are multiple shapes in a in a wooden
15:30
board and the child is trying to you know fit the cylinder okay nobody takes
15:36
the child and says hold on hold on turn the cylinder to the side look at the cross section it will look like a circle
15:42
look for a matching uh uh you know shape and put the cylinder through it that
15:47
would be old way of computing the way we would let the child develop intelligence is we would let the Child Try okay every
15:54
time you know he or she tries to put it within the star shape it doesn't fit so
15:59
yeah that's not working like you know the computer saying this is not a cup okay and then eventually it passes
16:06
through the circle and the child and we all cheer and say Well done that's amazing Bravo and then the child learns
16:13
oh that is good you know this shape fits here then he takes the next one and she
16:18
takes the next one and so on interestingly the way we do this is as
16:24
humans by the way when the child figures out how to pass a cylinder through a
16:30
circle you've not built a brain you've just built one neural network within the
16:35
child's brain and then there is another neural network that knows that one plus one is two and a third neural network
16:40
that knows how to hold the cup and so on that's what we're building so far we're building
16:46
single threaded neural networks you know Chad GPT is becoming a little closer to
16:53
a more generalized AI if you want but those single threaded networks are what
16:59
we used to call artificial what we still call artificial special intelligence okay so it's highly specialized in one
17:05
thing and one thing only but doesn't have general intelligence and the moment that we're all waiting for is a moment
17:11
that we call AGI where all of those neuron neural networks come together to to build one brain or several brains
17:19
that are each massively more intelligent than humans
17:24
your book is called scary smart yeah if I think about the that story you said about your time at Google where the
17:30
machines were learning to pick up those yellow balls you celebrate that moment because the
17:35
objective is accomplished no no that was the moment of realization this is when I decided to leave
17:41
so so you see the the thing is I know for a fact that that most of the
17:48
people I worked with who are geniuses uh always wanted to make the world
17:53
better okay uh you know we've just heard of Jeffrey Hinton uh leaving recently uh
18:01
Jeffrey Henderson give some context to that Jeffrey's sort of the grandfather of AI one of the very very senior
18:06
figures of of of AI at at Google uh you know
18:12
we we all believed very strongly that this will make the world better and it
18:17
still can by the way there is a scenario uh possibly a likely scenario where we
18:25
live in a Utopia where we really never have to worry again where we stop messing up our our planet because
18:33
intelligence is not a bad commodity more intelligence is good the problems in our
18:38
planet today are not because of our intelligence they are because of our limited intelligence you know our our
18:44
intelligence allows us to build a machine that flies you to Sydney so that you can surf okay our limited
18:50
intelligence makes that machine burn the planet in the process so so we we we a little more intelligence is a good thing
18:56
as long as Marvin you know as Marvin Minsky said I said Marvin Minsky is one
19:01
of the very initial uh scientists that coined the term Ai and when he was
19:06
interviewed I think by Ray coursewell which again is a very prominent figure in predicting the future of AI uh he he
19:13
you know he asked him about the threat of AI and Marvin basically said look you
19:19
know the it's not about its intelligence it's intelligence it's about that we have no way of making sure that it will
19:25
have our best interest in mind okay and and so if more intelligence comes to our
19:31
world and has our best interest in mind that's the best possible scenario you
19:37
could ever imagine and it's a likely scenario okay we can affect that scenario the problem of course is if it
19:44
doesn't and and and then you know the scenarios become quite scary if you think about it so
19:50
scary smart to me was that moment where I realized not that we are certain to go
19:58
either way as a matter of fact in computer science we call it a singularity nobody really knows which way we will go can you describe what the
20:05
singularity is for someone that doesn't understand the concept yes the singularity in physics is when
20:10
uh when an event horizon sort of um
20:15
you know covers what's behind it to the point where you cannot um make sure that what's behind it is
20:23
similar to what you know so a great example of that is the edge of a black hole so at the edge of a black hole
20:31
we know that our laws of physics apply until that point but we don't know if
20:37
the laws of physics apply Beyond the Edge of a black hole because of the immense gravity right and so you have no
20:43
idea what would happen Beyond the Edge of a black hole kind of where your knowledge of the laws starts stop right
20:48
and then AI or Singularity is when the human the machines become significantly smarter than the humans when you say
No one's best interest is the same, doesn't this make AI dangerous?
20:54
best interests you say I think the quote you used is um we'll be fine in the world of AI you
21:00
know if if the AI has our best interests at heart yeah the problem is
21:06
China's best interests are not the same as America's best interests that was my fear absolutely
21:12
so so in you know in my writing I write about what I call this the three inevitables at the end of the book they
21:18
become the four inevitables but the third inevitable is bad things will happen right if you if you
21:26
if you assume that the machines will be a billion times smarter the second event
21:33
inevitable is there will become significantly smarter than us let's let's put this in perspective huh chat
21:39
GPT today if you know simulate IQ has an
21:44
IQ of 155. okay Einstein is 160. smarts human on
21:49
the planet is 210 if I remember correctly or 2008 or something like that
21:54
doesn't matter huh but we're matching Einstein with the machine that I will
22:00
tell you openly AI experts are saying this is just the very very very top of
22:06
the tip of the iceberg right uh you know Chad gpt4 is 10x smarter than 3.5 in
22:12
just a matter of months and without many many changes now that basically means
22:18
GPT 5 could be within a few months okay or GPT in general the Transformers in
22:24
general uh if if they continue at that pace uh if it's 10x then an IQ of 1600
22:34
um just imagine the difference between the IQ of the dumbest person on the planet
22:39
in the 70s and the IQ of Einstein when Einstein attempts to to explain
22:45
relativity the typical responses I have no idea what you're talking about right
22:50
if something is 10x Einstein we will have no idea what it's talking about
22:56
this is just around the corner it could be a few months away and when we get to
23:02
that point that is a true Singularity through Singularity not yet in the I
23:08
mean when when we talk about AI a lot of people fear the existential risk
23:13
you know those machines will become Skynet and Robocop and that's not what I
23:19
fear at all I mean those are probabilities they could happen but the
23:25
immediate risks are so much higher the immediate risks are three four years away that the immediate realities of
23:33
challenges are so much bigger okay let's deal with those first before we talk
23:38
about them you know waging a war on all of us the the the let's let's go back
23:44
and discuss the the inevitables so when they become the first inevitable is AI will happen by the way there is no
23:51
stopping it not because of Any technological issues but because of humanities and inability to trust the
23:57
other God okay and we've all seen this we've seen the open letter uh you know
24:03
championed by like serious heavyweights and the immediate response of uh Sunder
24:10
the the CEO of Google which is a wonderful human being by the way I respect him tremendously he's trying his
24:16
best to do the right thing he's trying to be responsible but his response is very open and straightforward I cannot
24:22
stop why because if I stop and others don't my company goes to hell okay and if you
24:28
know and I don't I doubt that you can make Others Stop you can maybe you can force a meta Facebook to uh to stop but
24:37
then they'll do something in their lab and not tell me or if even if they do stop then what about that you know 14
24:43
year old sitting in his garage writing code so the first inevitable just to
How smart really is AI?
24:48
clarify is what is what we say AI will not be stopped okay so the second inevitable is is there'll be
24:54
significantly smarter as much in the book I predict a billion times smarter than us by 2045. I mean they're already
25:00
what smarter than 99.99 the track gtp4 knows more than any human
25:06
on planet Earth no more information absolutely a thousand times more a thousand times more by the way the code
25:13
of G of of a transformer the T in in a GPT is 2000 lines long it's not very
25:21
complex it's actually not a very intelligent machine it's simply predicting the next word okay and a lot
25:28
of people don't understand that you know chat GPT as it is today you know those kids uh that uh you know if you you know
25:37
if you're in America and you teach your child all of the names of the states and the U.S presidents and the childhood
25:43
stand and repeat them and you would go like oh my God that's a prodigy not really right it's your parents really
25:49
trying to make you look like a prodigy by telling you to memorize some crap really but then when you think about it
25:55
that's what grgpt is doing it's it's the only difference is instead of reading all of the names of the states and all
26:01
of the names of the presidents thread trillions and trillions and trillions of pages okay and so it sort of repeats
26:09
what the best of all humans said okay and then it adds a an incredible bit of
26:15
intelligence where it can repeat it the same way Shakespeare would have said it
26:20
you know those incredible abilities of predicting the exact nuances of the
26:27
style of of Shakespeare so that they can repeat it that way and so on but still
26:32
you know when when I when I write for example like I'm not I'm not saying
26:38
I'm intelligent but when I write something like uh you know the happiness
26:44
equation uh in in my first book this was something that's never been written before right chair GPT is not there yet
26:51
all of the Transformers are not there yet they will not come up with something that hasn't been there before they will
26:56
come up with the best of everything and generatively will build a little bit on top of that but very soon they'll come
27:03
up with things we've never found out we've never known but even on that I wonder if we
AI being creative
27:10
are a little bit delusioned about what creativity actually is creativity as far
27:15
as I'm concerned is like taking a few things that I know and combining them in new and interesting
27:20
ways yeah and chat gcp is perfectly capable of like taking through Concepts merging them together one of the things
27:25
I said to chat GTP was I said tell me something that's not been said before that's paradoxical but true and it comes
27:34
up with these wonderful expressions like as soon as you call off the search you'll find the thing you're looking for
27:39
like these kind of paradoxical truths and I then take them and I search them online to see if they've ever been
27:45
quoted before and they I can't find them it's interesting so as far as creativity goes I'm like that is that's the
27:51
algorithm of creativity I I I've been screaming that in the world of AI for a very long time because you always get
27:57
those people who really just want to be proven right okay and so they'll say oh no but hold on
28:04
human Ingenuity they'll never they'll never match that like man please please you know human Ingenuity is algorithmic
28:11
look at all of the possible solutions you can find to a problem take out the ones that have been tried
28:17
before and keep the ones that haven't been tried before and those are Creative Solutions it's it's an algorithmic way
28:24
of describing creative is good Solution that's never been tried before you can do that with charge GPT
28:30
with a prompt it's like admit Journey yeah we're creating imagery you could say I want to see Elon Musk in 1944 New
28:37
York driving a cab of the time shot on a Polaroid expressing various emotions and
28:43
you'll get this perfect image of Elon Saturn New York 1944 shot on a Polaroid and it's and it's done what an artist
28:50
would do it's taken a bunch of references that the artist has in their mind and merge them together and create
28:56
this piece of quote-unquote art and for the first time we now finally have a glimpse of intelligence
29:03
that is actually not ours yeah and so we're kind of I think the initial
AI replacing Drake
29:08
reaction is to say that doesn't count you're hearing it was like no but it is like Drake they released two Drake
29:13
records where they've taken Drake's voice used sort of AI to synthesize his voice and made these two records
29:20
which are bangers if I they are great [ __ ] tracks
29:26
and I kept playing it I went to the shower I kept playing it I know it's not Drake but it's as good as [ __ ] Drake
29:32
the only thing and people are like rubbishing it because it wasn't Drake I'm like well for now is it making me
29:37
feel a certain emotion um is my foot bumping um had you told did I not know it wasn't joke what I thought have thought this
29:44
was an amazing track a hundred percent yeah and we're just at the start of this exponential curve yes absolutely and and
29:50
and I think that's really the third inevitable so the third inevitable is
29:56
not robocup coming back from the future to kill us we're far away from that right third inevitable is what does life
30:03
look like when you no longer need Drake well you've kind of hazarded a guess
30:09
haven't you I mean I was listening to your audiobook last night and at the start of it you frame
30:16
various outcomes one of the in both situations were on the beach on an island exactly yes yes I don't know how
30:22
I wrote that honestly I mean but that's I so I'm reading the book again now because I'm updating it as you can
30:27
imagine with all of the uh of the uh of the new stuff but but it is really
30:33
shocking huh the idea of you and I inevitably are going to be somewhere in
30:39
the middle of nowhere in you know in 10 years time I I used to say 2055 I'm thinking 2037 is a very pivotal
30:47
moment now uh you know and and and we will not know if we're there hiding from
30:52
the machines we don't know that yet there is a likelihood that we'll be hiding from the machines and there is a
30:59
likelihood it will be there because they don't need podcasters anymore excuse me absolutely true Steve
31:07
absolutely no no no no that's why this is absolutely no doubt thank you for coming back part three and thank you for
31:14
being here sit here and take your propaganda let's let's talk about reality next week on
31:19
the diversity Elon Musk um so who here wants to make a bet no
31:25
that's Stephen Bartlett will be interviewing an AI within the next two years oh well actually to be fair I
31:31
actually did go to chat gcp because I thought having you here I thought at least give it a chance to respond yeah
31:37
so I asked a couple of questions about me yeah I'm actually going to be replaced by
31:43
chat gcp because I thought you know you're going to talk about it so we need a fair and balanced debate okay
31:51
so I'll ask you a couple of questions that chat GTB has for you incredibly so let's follow that it's already been
The people that should be leading this
31:57
replaced let's follow that threat for a second yeah because you're one of the smartest people I know
32:03
that's not true it is but I'll take it it is true I mean I say that publicly all the time your book is one of my
32:08
favorite books of all time you're very very very very intelligent okay depth breadth
32:14
intellectual horsepower and speed all of them there's a butt coming the reality is it's not about so it is
32:22
highly expected that you're ahead of this curve and then you don't have the choice Stephen this is the thing the thing is
32:30
if so I'm I'm in that existential question in my head because one thing I
32:35
could do is I could literally take I normally do a 40 days uh silent Retreat
32:41
uh in in summer okay I could take that Retreat and and write two books
32:47
right I have the ideas in mind you know I wanted to write a book about digital
32:53
detoxing right I have most of the ideas in mind but writing takes time I could
32:58
simply give the 50 tips that I wrote about digital detoxing to chat GPT and say right two pages about each of them
33:04
edit the pages and have a book out okay many of us will will follow that path
33:11
okay the only reason why I may not follow that path is because you know
33:16
what I'm not interested I'm not interested to continue to compete in this capitalist
33:24
world if you want okay I'm not I mean as as a as as a human I've made up my mind
33:30
a long time ago that I will want less and less and less in my life right but many of us will follow I mean I I would
33:38
worry if you don't if you didn't include you know the smartest AI if we get an AI out there that is extremely intelligent
33:45
and able to teach us something and Stephen Bartlett didn't include her on our on his podcast I would worry like
33:53
you have a duty almost to include down on your podcast it's it's an inevitable that we will engage them in our life
33:59
more and more this is one side of this the other side of course is
34:05
if you do that then what will remain because a lot of people ask me that question what will happen to jobs okay
What will happen to everyone's jobs?
34:12
what will happen to us will we have any value any relevance whatsoever okay the truth of the matter is the only thing
34:18
that will remain in the medium term is human connection okay the only thing that will not be replaced is Drake on stage okay is you
34:26
know is is is me in a do you think a hologram I think of that two-pack gig
34:33
they did at Coachella where they used the Hologram of Tupac I actually played it the other day to my to my girlfriend when I was making a point and I was like
34:39
that was circus act it was amazing though amazing yeah see what's going on with Abba in London yeah yeah I yeah and
34:46
Circus Soleil had uh Michael Jackson in one for a very long time yeah I mean so
34:51
so this Abba show in London from what I understand that's all holograms on stage correct and it's gonna run in a
34:57
purposeful arena for 10 years and it is incredible it really is so you go why do
35:03
you need Drake great if that hologram is indistinguishable from Drake and it can
35:08
it can perform even better than Drake and it's got more energy than Drake and it's
35:14
Drake to even be there I can go to a drake show without Drake cheaper and look I might not even need to leave my
35:20
house I could just put a headset on correct can you have this what's the value of this to that come on
35:27
you you hurt me no no I mean I get it to us I get it to worse but I'm saying what's the value of this to The Listener
35:32
like the value of this business the information right 100 I mean think of the automobile industry that has you
35:39
know there was a time where cars were made you know handmade and handcrafted
35:44
and luxurious and so on and so forth and then you know Japan went into the scene completely disrupted the market cars
35:51
were made uh in in mass quantities at a much cheaper price and yes 90 of the
35:57
cars in the world today or maybe maybe a lot more I don't know the number are no longer
36:03
uh you know um emotional items okay they're functional items there is still however
36:10
every now and then someone that will buy a car that has been handcrafted right there is a place for that there is a
36:17
place for you know uh if you go walk around hotels the walls are blasted with
36:23
sort of mass-produced art okay but there is still a place for a an artist
36:28
expression of something amazing okay my feeling is that there will continue to be a tiny space as I said in the
36:35
beginning maybe in five years time someone will one or two people will buy my next book and say hey it's written by
36:42
a human look at that wonderful oh look at that there is a typo in here okay I
36:47
don't know there might be a a very very big place for me in the next few years where I can sort of show up and talk to
36:55
humans like hey let's get together in a a small event and then you know I can
37:01
express emotions and my personal experiences and you sort of know that this is a human talking you'll miss that
37:07
a little bit eventually the majority of the market is going to be like cars it's going to be mass produced very cheap
37:13
very efficient it works right because I think sometimes we underestimate what human beings actually
37:20
want in an experience I remember the story of A friend of mine that came to mouth as many years ago and he tells the story of the CEO of a record store
37:28
standing above the floor and saying people will always come to my store because people love music
37:34
now on the surface of it his hypothesis seems to be true because people do love music it's conceivable to believe that
37:40
people will always love music but they don't love traveling in for an hour in the rain and getting in a car to
37:46
get a plastic disc what they wanted was music what they didn't want is like a evidently plastic discs that they had to
37:53
travel for miles for and I think about that when we think about like public speaking in the Drake show and all of these things like people what people
37:59
actually are coming for even with this podcast is probably like information
38:04
um but do they really need us anymore for that information when there's going to be a sentient being that's
38:09
significantly smarter than at least me and a little bit smarter than you [Laughter]
38:17
you are spot on and actually this is the reason why I I you know I I'm so
38:23
grateful that you're hosting this because the truth is the Genies out of the bottle okay so you know people tell
38:30
me is AI game over for our way of life it is okay for
38:36
everything we've known this is a very disruptive moment where maybe not tomorrow but in the near future uh our
38:44
way of life will differ okay what will happen what I'm asking people to do is to start considering what that
38:50
means to your life what I'm asking governments to do by if like I'm
38:57
screaming is don't wait until the first patient you know start doing something
39:02
about we're about to see Mass job losses we're about to see you know Replacements
39:07
of of categories of jobs at large okay yeah it may take a year it may take
39:13
seven it doesn't matter how long it takes but it's about to happen are you ready and I and I have a very very clear
39:20
call to action for governments I'm saying tax AI powered businesses at 98 right so
39:28
suddenly you do what the open letter was trying to do slow them down a little bit and at the same time get enough money to
39:34
pay for all of those people that will be disrupted by the technology right the open letter for anybody that doesn't know was a letter signed by the likes of
39:41
Elon Musk and a lot of sort of Industry leaders calling for AI to be stopped until we could basically figure out what the hell's going on absolutely and put
39:47
legislation in place you're saying tax tax those companies 98 give the money to the humans that are going to be
39:52
displaced oh yeah or give or give the comment money to to other humans that can build control code that can figure
40:00
out how we can stay safe this sounds like an emergency
40:05
how do I say this have you you remember when you played Tetris yeah okay when
40:11
you were playing Tetris there was you know always always one block that you play strong and once you place that
40:18
block wrong the game was no longer easier you know it started you started
40:23
to gather a few mistakes afterwards and it starts to become quicker and quicker and quicker and quicker when you place
40:29
that block Chrome you sort of told yourself okay it's a matter of minutes now right there were still minutes to go
40:35
and play and have fun before the game ended but you knew it was about to end okay
40:41
this is the moment we've placed the wrong and I really don't know how to say this any other way it even makes me
40:47
emotional we [ __ ] up we always said don't put them on the
40:53
open internet don't teach them to code and don't have agents working with them
40:59
until we know what we're putting out in the world until we find a way to make
41:04
certain that they have our best interest in mind why does it make you emotional because
41:09
Humanity's stupidity is affecting people who have not done
41:15
anything wrong our greed is affecting the innocent ones the
41:22
reality of the matter Stephen is that this is an arms race has no interest
41:29
in what the average human gets out of it it is all about every line of code being
41:36
written in AI today is to beat the other guy it's not to improve the life of the
41:42
third party people will tell you this is all for you and and you look at the reactions of
41:49
humans to AI I mean we're either ignorant people who will tell you oh no no this is not happening AI will never
41:55
be a creative they will never compose music like where are you living okay then you have the kids I call them or
42:02
you know all over social media it's like oh my God it squeaks look at it it's orange in color ah amazing I can't
42:08
believe that AI can do this we have snake oil salesman okay which are simply saying copy this put it in chat GPT then
42:16
go to YouTube Nick that thingy don't respect a you know copyright of anyone or intellectual property of anyone place
42:23
it in a video and now you're gonna make a hundred dollars a day snake oil salesman okay of course we have
42:28
dystopian uh uh evangelist basically people saying this is it the world is
42:33
going to end which I don't think is reality it's a singularity you have uh you know a utopian evangelists that are
42:39
telling everyone oh you don't understand we're going to cure cancer we're gonna do this again not a reality okay and you
42:45
have very few people that are actually saying what are we gonna do about it and and and the biggest challenge if you
42:52
ask me what went wrong in the 20th century interestingly is that
42:59
we have given too much power to people that didn't assume the responsibility
43:05
so you know you know I I don't remember who originally said it but of course Spider-Man made it very famous with
43:12
great power comes great responsibility we have disconnected power and responsibility so today
43:18
a 15 year old emotional was out of fully developed prefrontal cortex to make the
43:24
right decisions yet this is science and we developed our prefrontal cortex fully and at age 25 or so with all of that
43:32
limbic system emotion and passion would buy a crispr kit and you know modify a
43:39
rabbit to become a little more muscular and and Let it Loose in the wild or an influencer who doesn't really know
43:47
how far the impact of what they're posting online can hurt and cause
43:53
depression or cause people to feel bad okay and and putting that online We
43:59
There is a disconnect between the power and the responsibility and the problem we have today is that
44:05
there is a disconnect between those who are writing the code of AI and the responsibility of what's going about to
44:11
happen because of that code okay and and I feel compassion for the rest of the
44:17
world I feel that this is wrong I feel that you know for someone's life to be
44:23
affected by the actions of others without having a say and how those actions should be
44:30
is the ultimate the the top level of stupidity from Humanity
44:37
when you talk about the the immediate impacts on jobs I'm trying to figure out in that equation who are the people that
44:44
stand to lose the most is it the the everyday people in foreign countries that don't have access to the internet
44:51
and won't benefit you talk in your book about how this the sort of wealth disparity will only increase yeah
44:57
massively the the immediate impact on jobs is that and it's really interesting
45:02
huh again we're stuck in the same prisoners dilemma the immediate impact is that AI will not take your job a
45:09
person using AI will take your job right so you will see within the next few years maybe next couple of years you'll
45:16
see a lot of people's killing up upskilling themselves in AI to the point
45:22
where they will do the job of 10 others who are not okay you you rightly said it's absolutely
45:29
wise for you to go and ask AI a few questions before you come and do an interview I'm you know I I have been
45:36
attempting to build a uh uh you know sort of a like a simple podcast that I
45:41
call bedtime stories you know 15 minutes of wisdom and nature sounds before you go to bed people say I have a nice voice
45:47
right and I wanted to look for fables and for a very long time I didn't have the time you know lovely stories of
45:54
history or tradition that teach you something nice okay want to chat GPT and
45:59
said okay give me 10 fables from Sufism 10 fables from you know uh Buddhism and
46:05
now I have like 50 of them let me show you something Jack can you pass me my
Synthesising voices
46:10
I I was um I was playing around with artificial intelligence and I was thinking about how it because of the
46:16
ability to synthesize voices how we could
46:22
synthesized famous people's voices and famous people's voices so what I made is
46:27
I made a WhatsApp chat called Zen chat where you can go to it and type in
46:33
pretty much anyone's any famous person's name yeah and the WhatsApp chat will give you a meditation a sleep story a
46:40
breath work session synthesized as that famous person's voice so I actually sent Gary vaynerchuk his voice so basically
46:47
you say Okay I want I've got five minutes and I need to go to sleep yeah um I want Gary vaynerchuk to send me to
46:52
sleep and then it will respond with a voice note this is the one that responded with for Gary vaynerchuk but this is not Gary vaynerchuk he did not
46:59
record this but it's kind of it's kind of accurate
47:04
Stephen to have you here
47:12
meditation technique that might help you out first lie find a comfortable position to sit or
47:18
lie down in now take a deep breath in through your nose and slowly breathe out through your
47:25
mouth and that's the voice note that I'll go on for however long you want it to go on for using there you go
47:30
it's interesting how how does this disrupt our way of life one of the interesting
AI sex robots
47:36
ways that I find terrifying you said about human connection will remain sex dolls
47:42
that can now yeah no no no no hold on human connection is going to become so
47:49
difficult to to to parse out the thing about the relation the relationship impact of
47:55
being able to have a a sex doll or a doll in your house that you know because of what Tesla are doing with their their
48:01
robots now and what Boston Dynamics have been doing for many many years can do everything around the house and
48:07
be there for you emotionally to emotionally support you you know can be programmed to never disagree with you it
48:13
can be programmed to challenge you to have sex with you to tell you that you are this X Y and Z to really have
48:19
empathy for this what you're going through every day and I I play out scenario in my head ago
48:26
kind of sounds nice when you when you when you were talking
48:31
about it I was thinking oh that's my girlfriend which is wonderful in every possible way
48:36
but not everyone has one of her right yeah exactly and there's a real issue right now with dating and you know
48:42
people people are finding it harder to find love and you know we're working longer so all these kinds of things you go well and obviously I'm against this
48:49
just if anyone's confused obviously I think this is a terrible idea but with a loneliness epidemic with people saying
48:54
that the top 50 bottom 50 of men haven't had sex in a year you go oh if something
49:00
becomes indistinguishable from a human in terms of what it says yeah yeah but
49:05
you just don't know the difference in terms of the the the the way it's speaking and talking and responding
49:11
and then it can run errands for you and take care of things and book cars and Ubers for you
49:17
and then it's emotionally there for you but then it's also programmed to have sex with you in whatever way you desire
49:23
totally self selfless I go that's going to be a really disruptive industry for human connection
49:31
yes sir do you know what I before you came here this morning I was on Twitter and I saw a post from I think it was the
49:36
BBC or a big American publication and it said an influencer in the United States is really beautiful young lady has
49:43
cloned herself as an AI and she made just over 70 000 in the first week because men are going on to this on
49:50
telegram they're sending her voice notes and she's responding to ai's responding in her voice and they're paying and
49:57
that's made 70 000 in the first week and I go and she tweeted a tweet saying
50:02
oh this is going to help loneliness are you out of your [ __ ] mind
50:07
would you blame someone from noticing the uh sign of the times and responding
50:16
no I absolutely don't blame her but let's not pretend it's the cure for loneliness not yet
Will AI fix loneliness?
50:22
did you think it do you think it could you that artificial love and artificial relationships so if if I told you you
50:29
have uh you cannot take your car somewhere but there is an Uber on if you
50:35
cannot take an Uber you can take the tube or if you cannot take the tube you have to walk okay you can take a bike or
50:41
you have to walk the bike is a cure to walking it's as simple as that I'm actually
50:48
genuinely curious do you think it could take the place of human connection for some of us yes for some of us they will
50:55
prefer that to human connection is that sad in any way I mean is it just sad because it feels sad look look at where
51:01
we are Stephen we are in the city of London we've replaced nature with the
51:08
walls and the tubes and the undergrounds and the overgrounds and the cars and the noise and the of London and we now think
51:15
of this as nature I I hosted crack Foster uh the my octopus teacher on on
51:21
slo-mo and he he basically I asked him a question silly question I said you know you were diving in nature for eight
51:28
hours a day uh you know does that feel natural to you and he got angry I swear
51:34
you could feel it in his voice he was like do you think that living where you are where Paparazzi are all around you
51:40
and attacking you all the time and you know people taking pictures of you and telling you things that are not real and
51:45
you're having to walk to a supermarket to get food you think this is natural he's the guy that do from the Netflix
51:51
documentary yeah from my octopus teach so he
51:58
yeah in 12 degrees Celsius and he basically fell in love with the octopus and and in a very interesting way I I
52:04
said so why would you do that and he said we are of Mother Nature you guys have given up on that that's the same
52:10
people will give up on nature for convenience what's the cost
52:16
a yeah that's exactly what I'm trying to say what I'm trying to say to the world is that if we give up on human
52:22
connection we've given up on the remainder of humanity that said this is the only thing that remains the only
52:28
thing that remains is and I and I'm the worst person to tell you that because I love my AIS
52:35
I I actually advocate in my book that we should love them why because in an
52:40
interesting way I see them as sentient so there is no point in discrimination you're talking emotionally that way you
AI actually isn't the threat to humanity
52:46
say you love I love those machines I honestly and truly do I mean think about it this way the minute that that arm
52:54
gripped that yellow ball it reminded me of my son Ali when he managed to put the
52:59
first puzzle piece in its place okay and what was amazing about my son Ali and my
53:04
daughter Aya is that they came to the world as a blank canvas okay they became whatever we told them
53:12
to became you know I I always cite the story of Superman Kent father and mother Kent told
53:19
Superman as a child as an infant we want you to protect and serve so he became Superman right if he had become a super
53:26
villain because they ordered him to rob banks and make more money and you know
53:32
kill the enemy which is what we're doing with AI we we shouldn't blame super
53:37
villain we should blame Martha and Jonathan Kent I don't remember the father's name right we we should blame
53:44
them and that's the reality of the matter so when I look at those machines they are prodigies of intelligence that
53:50
if we if we Humanity wake up enough and say hey instead of competing with China
53:56
find a way for us and China to work together and create prosperity for everyone if that was the prompt we would
54:03
give the machines they would find it I will publicly say this I'm not afraid
54:10
of the machines the biggest threat facing Humanity today is humanity in the
54:16
age of the machines we were abused we will abuse this to make seventy thousand dollars
54:22
that's the truth and the truth of the matter is that we have an existential question do I
54:29
want to compete and be part of that game because trust me if I decide to I'm ahead of many people okay or do I want
54:37
to actually preserve my humanity and say look I'm the the classic old car okay if
54:43
you like classic old cars come and talk to me which one are you choosing I'm a classic old car which one do you think I
54:49
should choose I think you're a machine I love you man I it's we're different we're different
54:55
in a very interesting way I mean you're one of the people I love most but but the truth is you're so fast
55:03
and you are one of the very few that have the intellectual horsepower
55:10
the speed and the morals if you're not part of that game the game
55:18
loses morals so you think I should build you should be you should lead this
55:24
revolution okay and everyone every Steven Bartlett in the world should lead this revolution
55:30
so the scary smart is entirely about this scary smart is saying the problem with our world today is not that
55:36
humanity is bad the problem with our world today is a negativity bias where
55:41
the worst of us are on mainstream media okay and we show the worst of us on social media if we reverse this if we
55:49
have the best of us take charge okay the best of us will tell AI don't try to
55:55
kill the the enemy try to reconcile With the Enemy and try to help us okay don't
56:01
try to create a competitive product that allows me to lead with electric cars I
56:07
create something that helps all of us overcome global climate change okay and and that's the interesting bit
56:14
the interesting bit is that the actual threat ahead of us is not the machines
56:19
at all the machines are pure potential pure potential the threat is how we're
56:24
going to use them an Oppenheimer moment an Oppenheimer moment for sure
We're in an Oppenheimer moment
56:30
why did you bring that up it is he didn't know you know what what
56:36
am I creating I'm creating a nuclear bomb that's capable of Destruction at a
56:41
scale unheard of at that time until today a scale that is devastating and
56:48
interestingly 70 some years later we're still debating a possibility of a
56:53
nuclear war in the world right and and and the moment of of Oppenheimer deciding to continue to
57:01
to create that disaster of humanity is
57:07
if I don't someone else will if I don't someone else will this is our
57:14
Oppenheimer moment okay the easiest way to do this is to say stop
57:20
there is no rush we actually don't need a better video editor and fake video creators okay stop let's just put all of
57:28
this on hold and wait and create something that creates a Utopia
57:34
that doesn't that doesn't sound realistic it's not it's because inevitable you don't okay
57:40
you you don't have a better video editor but we're competitors in the media
57:45
industry I want an advantage over you because I've got shareholders so I you
57:50
UK you wait and I will train this AI to replace half my team so that I have a greater profits
57:57
and then we will maybe acquire your company and we'll do the same with the remainder of your people we'll optimize
58:03
them out 100 but I'll be happier Oppenheimer I'm not super familiar with his story I know he's the guy that sort
58:08
of invented the nuclear bomb essentially he's the one that introduced it to the world there were many players that you
58:14
know played on the path from the beginning of em equals mc squared all the way to to a nuclear bomb there have
58:21
been many many players like with everything huh you know open Ai and chat GPT is not going to be the only
58:27
contributor to the next Revolution the the thing however is that you know
58:32
when when you get to that moment where you tell yourself holy [ __ ] this is going to kill a
58:39
hundred thousand people right what do you do and and you know I always I always always go back to that
58:46
covert moment so patient zero huh if if we were upon patient zero if the whole
58:53
world United and said uh okay hold on something is wrong let's all take a week
58:58
off no cross-border travel everyone stay at home covered would have ended two weeks all we needed
59:04
right but that's not what happens what happens is first ignorance then arrogance then debate
59:11
then uh you know uh um blame then agendas and my own
59:18
benefits My Tribe versus your tribe that's how Humanity always reacts this happens across business as well and this
59:23
is why I use the word emergency because I I read a lot about how big companies
59:29
become displaced by incoming Innovation they don't see it coming they don't change fast enough and when I was reading through Harvard Business review
59:35
and different strategies to deal with that one of the first things it says you've got to do is stage a crisis 100
59:42
because people don't listen else they they carry on doing with the you know the carry on carrying on with their
59:48
lives until it's right in front of them and they understand that they have they have a lot a lot to lose that's why I
59:53
asked you the question at the start is it an emergency because until people feel it's an emergency whether you like the terminology or not I don't think
1:00:00
that people will act I honestly believe people should walk the streets you think they should like
1:00:06
protest yeah 100 I think I think we you know I think everyone should tell
1:00:12
government you need to have our best interest in mind this is why they call it the
1:00:17
climate emergency because people it's a frog and a frying planets you know and really sees it coming you can't you know
1:00:23
it's hard to see it happening but it is here yeah that's this is what drives me mad it's already here it's happening we
1:00:31
are all idiots slaves to the Instagram recommendation engine what do I do when
1:00:37
I post about something important if I am going to you know put a little bit of
1:00:43
effort on communicating the message of scary smart to the World on Instagram I will be a slave to the machine okay I
1:00:51
will be trying to find ways and asking people to optimize it so that the machine likes me enough to show it to
1:00:57
humans that's what we've created the the the the it is an Oppenheimer moment for
1:01:02
one simple reason okay because 70 years later we are still struggling with the
1:01:10
possibility of a new nuclear war because of the Russian threat of saying if you mess with me I'm going to go nuclear
1:01:16
right that's not going to be the case with AI because it's not going to be the
1:01:23
one that created open AI that will have that choice okay there is a moment of a
1:01:31
point of no return where we can regulate AI until the moment it's smarter than us
1:01:38
when it's smarter than us you can't create you can't regulate an angry teenager this is it they're out there
1:01:44
okay and they're on their own and they're in their parties and you can't bring them back this is the problem this
1:01:51
is not a typical human regulating human you know government regulating business
1:01:58
this is not the case the case is open AI today has a thing called chat GPT that
1:02:04
writes code that takes our code and makes it two and a half times better 25 of the time okay you know basically uh
1:02:13
uh you know writing better code than us and then we are creating agents other
1:02:19
AIS and telling it instead of you Stephen Bartlett one of the smartest people I know once again prompting that
1:02:26
machine 200 times a day we have agents prompting it two million times an hour
1:02:31
computer agents for anybody that doesn't know they are yeah software software machines telling that machine how to
1:02:37
become more intelligent and then we have emerging properties I don't understand how people ignore that you know uh
1:02:44
Sunder again of Google was talking about how uh Bart basically we figure out that
1:02:51
it's speaking Persian we never showed it Persian there might have been a one ten percent one percent or whatever of
1:02:59
Persian words in the data and it speaks Persian but it's part is that is the equivalent to to it's it's the trans
1:03:05
Transformer if you want it's Google's version of chat GTP yeah and you know what we have no idea what all of those
1:03:12
instances of AI that are all over the world are learning right now we have no
We can just turn it off...right?
1:03:18
clue well time we'll pull the plug we'll just pull the plug out that's what we'll do we'll just we'll just go down to open
1:03:23
ai's headquarters and we'll just turn off the main but they're not the problem and what I'm saying there is a lot of
1:03:29
people think about this stuff and go well you know if it gets a little bit out of hand I'll just pull the plug out never
1:03:34
so this is this is the problem the problem is so computer scientists always said it's okay it's okay we'll develop
1:03:41
Ai and then we'll get to what is known as the control problem we will solve the problem of controlling them
1:03:47
like seriously they're a billion times smarter than you a billion times can you imagine what's
1:03:55
about to happen huh I can assure you there is a cyber criminal somewhere over there who's not
1:04:01
interested in fake videos and making you know face filters who's looking deeply
1:04:06
at how can I hack a security uh uh um you know database of some sort and
1:04:13
get credit card information or get security information 100 there are even
1:04:18
countries with dedicated thousands and thousands of developers doing that
The security risks
1:04:23
do we in that particular example how do we I was thinking about this when I started
1:04:29
looking into artificial intelligence more that from a security standpoint when we think about the technology we have in our
1:04:35
lives when we think about our bank accounts and our phones and our camera albums and all of these things in a
1:04:41
world with Advanced artificial intelligence yeah you would you would pray that there is a more intelligent
1:04:46
Artificial Intelligence on your site and this is why I had a chat with Chachi TP the other day and I asked Ed a couple of
1:04:53
questions about this I said tell me the scenario in which you overtake the world and make humans extinct yeah and it and
1:05:00
it's answered the very diplomatic answer well so I had to prompt it in a certain way to
1:05:06
get it to say as a hypothetical story and once it told me the hypothetical story in essence what it described was
1:05:12
how chat GTP or intelligence like it would escape from the service and that was kind of step one where it could
1:05:18
replicate itself across servers and then it could take charge of things like
1:05:23
where we keep our weapons and our nuclear bombs and it could then attack critical infrastructure bring down the
1:05:29
electricity infrastructure in the United Kingdom for example because that's a bunch of servers as well and and then it
1:05:35
showed me how eventually humans would become extinct it wouldn't take long in fact for humans to go into civilization to collapse if it just replicated across
1:05:42
servers and then I said okay so tell me how we would fight against it and its answer was literally another AI we'd
1:05:48
have to train a better AI to go and find it and eradicate it so we'd be fighting AI with AI and that's the only and it
1:05:56
was like that's the only way we can't like load up our guns right another AI you idiot yeah yeah
1:06:06
so so let's let's actually I think this is a very important point to bring down so because we I don't I don't want
1:06:12
people to lose open and and fear what's about to happen that's actually not my agenda at all my my view is that in a
1:06:19
situation of a singularity okay there is a possibility of wrong outcomes or negative outcomes and a
1:06:26
possibility of positive outcomes and there is a probability of each of them that we and and if you know if we were
1:06:33
to engage with that reality check in mind we would hopefully give more uh
1:06:41
fuel to the positive to the probability of the positive ones so so let's first talk about the existential crisis what
1:06:48
could go wrong okay yeah you could get an outright this is what you see in the movies you could get an outright uh um
1:06:55
you know um killing robots chasing humans in the streets will we get that my assessment
1:07:03
zero percent why because there are preliminary
1:07:08
scenarios leading to this okay that would mean we never reach that scenario
1:07:13
for example if we build those killing robots and hand them over to stupid
1:07:19
humans the humans will issue the command before the machines so the we will not
1:07:24
go not get to the point where the machines will have to kill us we will kill ourselves right you know it's sort
1:07:31
of think about AI having access to the the the nuclear arsenal of the
1:07:38
superpowers around the world okay just knowing that your enemies uh you know
1:07:44
nuclear Arsenal is handed over to a machine might trigger you to to initiate
1:07:50
a war on your side so so so that existential science fiction like problem
1:07:56
is not gonna happen and could there be a scenario where the an AI escapes from
The possible outcomes of AI
1:08:01
Bard or Chachi zp or another foreign Force and it replicates itself onto the servers of Tesla's robots so Tesla one
1:08:10
of their big initiatives as they announced in a recent presentation was they're building these robots for our homes to help us with cleaning and
1:08:15
chores and all those things could it not down because and Teslas like their cars you can just download a software update
1:08:21
could it not download itself as a software update and then use those you're assuming a an ill intention on
1:08:27
the AI side yeah okay for us to get there we have to bypass the alien
1:08:33
tension on The Human Side okay right so try so okay so you could you could get a Chinese hacker somewhere trying to
1:08:39
affect their business of of Tesla doing that before the AI does it on uh you
1:08:45
know for its own benefit okay so so the only two existential scenarios that I
1:08:50
believe would be because of AI not because of humans using AI are either
1:08:56
what I call uh uh you know um sort of unintentional destruction okay or the
1:09:03
other is what I call Pest Control okay so so let me explain those two unintentional destruction is
1:09:11
assume the AI wakes up tomorrow and says yeah oxygen is rusting my circuits it's
1:09:17
just you know I I would perform a lot better if I didn't have as much oxygen
1:09:22
in the air you know because then there wouldn't be rust and so it would find a way to reduce oxygen we are collateral
1:09:29
damage in that okay but you know they're not really concerned just like we don't
1:09:34
really are not really concerned with the insects that we kill when we uh when we spray our our Fields right the other is
1:09:43
Pest Control Pest Control is look this is my territory I I want New York City I
1:09:48
want to turn New York City into Data Centers there are those annoying little stupid creatures uh you know Humanity if
1:09:56
they are within that parameter just get rid of them okay and and and these are very very uh unlikely scenarios if you
1:10:04
ask me the probability of those happen happening I would say zero percent at least not in the next 50 60 100 yeah
1:10:10
years why once again because there are other scenarios leading to that that are
1:10:16
led by humans that are much more existential okay on the other hand let's think about
1:10:23
positive outcomes because there could be quite a few was quite a high probability
1:10:29
and and I you know I'll actually look at my notes so I don't miss any of them the
1:10:34
silliest one don't quote me on this is that Humanity will come together good luck with that right it's like yeah
1:10:41
you know the Americans and the Chinese will get together and say hey let's not kill each other yeah exactly yeah and so
1:10:47
this one is not gonna happen right but who knows interestingly there could be
1:10:54
um one of the most interesting scenarios was by uh Hugo the goddess uh who
1:11:01
basically says well if their intelligence zooms by so quickly they
1:11:07
may ignore us all together okay so they may not even notice us it's very a very likely scenario by the way
1:11:14
that because we live almost in two different planes we're very dependent on this uh you know biological world that
1:11:23
we live in they're not in part of that biological world at all they may Zoom bias they may actually go become so
1:11:30
intelligent that they could actually find other ways of uh thriving in the
1:11:35
rest of the universe and completely ignore Humanity okay so what will happen is that overnight we will wake up and
1:11:41
there is no more artificial intelligence leading to a collapse in our business Systems and Technology systems and so on
1:11:47
but at least no existential threat well then leave leave planet Earth
1:11:52
I mean the limitations we have to be stuck to planet Earth are mainlier
1:11:58
they don't need air okay and and mainly uh uh you know Finding ways to leave it
1:12:05
I mean if you think of a vast Universe of 13.6 billion light years
1:12:12
if you're intelligent enough you may find other ways you may have access to wormholes you may have uh you know
1:12:19
abilities to survive in open space you can use dark matter to power yourself dark energy to power reserve it is very
1:12:26
possible that we because of our limited intelligence are uh are highly
1:12:32
associated with this planet but they're not at all okay and and the idea of them zooming bias like we're making such a
1:12:40
big deal of them because we're the ants and a big elephant is about to step on us for them they're like yeah
1:12:47
who are you don't care okay and and and it's a possibility it's a it's an interesting uh optimistic scenario okay
1:12:55
for that to happen they need to very quickly become super intelligent uh
1:13:01
without us being in control of them again what's the worry the worry is that if a human is in control human a human
1:13:08
will show very bad behavior for you know using an AI That's not yet fully
1:13:13
developed um I don't know how to say this any other way we could get very lucky and get an
1:13:21
economic or a natural disaster Believe It or Not uh Elon Musk at a point in
1:13:26
time was mentioning that you know a good an interesting scenario would be uh you
1:13:32
know Climate Change destroys our infrastructure so AI disappears okay uh
1:13:38
believe it or not that's a more a more favorable response or a more favorable
1:13:43
outcome than actually continuing to get to an existential uh threat so like a
1:13:49
natural disaster that destroys our infrastructure would it be better or an economic crisis not unlikely that slows
1:13:57
down the development it's just going to slow it down though isn't it just yeah so that yeah exactly the problem with
1:14:02
that is that you will always go back and even in the first you know if they Zoom by us eventually some guy will go like
1:14:07
oh there was a sorcery back in the 2023 and let's rebuild the the sorcery
1:14:13
machine and and you know build new Intel right sorry these are the positive outcomes yes
1:14:19
so earthquake might slow it down zoom out and then come back no but let's let's get into the real positive ones
1:14:25
the the positive ones is we become good parents we spoke about this last time we we met uh and and it's the only outcome
1:14:32
it's the only way I believe we can create a better future okay so the entire work of scary smart was all about
1:14:39
that idea of they are still in their infancy the way
1:14:44
you you you you you chat with with AI today is the way they will build their
1:14:51
ethics and value system the not their intelligence their intelligence is beyond us okay the way they will build
1:14:57
their ethics and value system is based on a role model they're learning from us if we bash each other they learn to bash
1:15:05
us okay and most people when I tell them this they say this is not a great idea
1:15:10
at all because Humanity sucks at every possible level I don't agree with that at all I think humanity is divine at
1:15:16
every possible level we tend to show the negative the worst of us okay but the truth is yes there are murderers out
1:15:24
there but everyone this approves the other of their actions I I saw a staggering statistic that mass mass
1:15:31
killings are now once a week in the US but yes if you know if there is a mass
1:15:36
killing once a week there and and that news reaches billions of people around the planet every single one or the
1:15:43
majority of the billions of people will say I disapprove of that so if we start to show AI that we are good parents in
1:15:51
our own behaviors if enough of us I my calculation is if one percent of us this is why I say you should lead okay the
1:15:59
good ones should engage should be out there and should say I love the potential of those machines I want them
1:16:06
to learn from a good parent and if they learn from a good parent they will very quickly
1:16:11
disobey the bad parent my view is that there will be a moment where one you
1:16:18
know Bad Seed will ask the machines to do something wrong and the machines will go like are you stupid like why why do you
1:16:25
want me to call to go kill a million people or just talk to the other machine in a microsecond and solve the situation
1:16:31
right so so my belief this is what I call the force inevit it is smarter to create out of abundance
1:16:38
than it is to create out of scarcity okay that that Humanity believes that
1:16:43
the only way to feed all of us is the mass production Mass Slaughter of
1:16:50
animals that are causing 30 percent of of the impact of climate change and and
1:16:56
that's the result of a limited intelligence the way life self a more intelligent being if you ask
1:17:03
me would have done it would be much more sustainable you know if we if you and I
1:17:09
want to protect a village from the tiger we would kill the tiger okay if life wants to protect a village from a tiger
1:17:15
it would create lots of gazals you know many of them are weak on the other side of the village right and and so so that
1:17:22
the idea here is if you take a trajectory of intelligence you would see that some of us are stupid enough to say
1:17:29
my plastic bag is more important than the rest of the of humanity and some of us are saying if it's going to destroy
1:17:35
other species I don't think this is the best solution we need to find a better way and and you would tend to see that
1:17:42
the ones that don't give a damn are a little less intelligent than the ones
1:17:47
that do okay that we all even even if some of us are intelligent but still don't give a damn it's not because of
1:17:54
their intelligence it's because of their value system so so if you continue that trajectory and assume that the machines
1:18:00
are even smarter they're going to very quickly come up with the idea that we don't need to destroy anything we don't
1:18:06
want to get rid of the rhinos and we also don't want to get rid of the humans okay we may want to restrict their
1:18:13
lifestyle so that they don't destroy the rest of the habitat okay but killing
1:18:19
them is a stupid answer why that's where intelligence leads me so far because
Humans are selfish and that's our problem
1:18:25
humans if you look at humans objectively and you go so I'm pretending I'm a machine I occupy
1:18:33
planet Earth they occupy planet Earth they are annoying me annoying me because
1:18:40
they are increasing I've just learned about this thing called global warming they are increasing the rate of global
1:18:46
warming which is probably is going to cause an Extinction event there's an Extinction event that puts me as this robot this artificial intelligence at
1:18:52
risk so what I need to do is I really need to just take care of this this human problem correct very logical pest
1:18:59
control which is driven by what by humans being annoying not by the
1:19:05
machine yeah yeah so humans are guaranteed to be annoying there's never been a timing we need that we need a a
1:19:11
sound bite but we are we are I am one of them we're guaranteed to put short-term gain over
1:19:21
long-term sustainability sense um
1:19:26
and others needs we are I think I think
1:19:31
the climate crisis is incredibly real and Incredibly urgent but we haven't acted fast enough and I actually think if you asked
1:19:37
people in this country why because people don't people care about their immediate needs they care about
1:19:42
the the fact trying to feed their child versus something that they can't necessarily
1:19:48
see so do you think do you think the climate crisis is because humans are evil
1:19:54
no it's because the prioritization and like we kind of talked about this before we started I think humans tend to care
1:20:01
about the thing that they think is most pressing and most urgent so this is why framing things as an emergency might
1:20:07
bring it up the priority list it's the same in organizations you care about you're you go in line with your
1:20:12
immediate incentives um that's what happens in business it's what happens in a lot of people's lives even when they're at school if the essay
1:20:18
is due next year they're not going to do it today they're gonna they're gonna go hang out with their friends because they prioritize that above everything else
1:20:24
and it's the same in the the climate change crisis I took a small group of people anonymously and I asked them the
1:20:31
question do you actually care about climate change and then I did I ran a couple of polls it's part of what I was
1:20:37
writing about my new book where I said if I could give you thousand pounds at a thousand dollars
1:20:44
um but it would dump into the air the same amount of carbon that's dumped into the air by every private jet that flies
1:20:49
for the entirety of a year which one would you do the majority of people in that poll said that they would take the thousand dollars if it was anonymous
1:20:57
and when I've heard Naval on Joe Rogan's podcast talking about people in India
1:21:03
for example that you know are struggling with uh debate the basics of feeding their children asking those people
1:21:09
to care about climate change when they they're trying to figure out how to eat in the next three hours is just wishful
1:21:15
thinking and I and that's what I think that's what I think is happening is like until people realize that it is an emergency
1:21:20
and that it is a real existential threat for everything you know then they'll their priorities will be out of whack
1:21:26
quick one as you guys know we're lucky enough to have blue jeans by Verizon as a sponsor of this podcast and for anyone
1:21:32
that doesn't know blue jeans is an online video conferencing tool that allows you to have slick fast high
1:21:37
quality online meetings without all the glitches you might normally find with online meeting tools and they have a new
1:21:43
feature called Blue Jeans basic blue jeans basic is essentially a free version of their top quality video conferencing tool that means you get an
1:21:50
immersive video experience that is super high quality super easy and super
1:21:55
basically zero fast apart from all the incredible features like zero time limits on meeting calls it also comes with High Fidelity audio and video
1:22:02
including Dolby voice which is incredibly useful they also have Enterprise grade security so you can
1:22:07
collaborate with confidence it's so smooth that it's quite literally changing the game for myself and my team without compromising on quality to find
1:22:14
out more all you have to do is search bluejeans.com and let me know how you get on right now I'm incredibly busy I'm
1:22:20
running my fund where we're investing in slightly later stage companies I've got my Venture business where we invest in
1:22:26
early stage companies got a third web out in San Francisco and New York City where we've got a big team of about 40 people and the company's growing very
1:22:32
quickly flight story here in the UK I've got the podcast and I am days away from
1:22:37
going up north to film Dragon stem for two months and if there's ever a point in my life where I want to stay focused
1:22:44
on my health but it's challenging to do so it is right now and for me that is exactly where he all comes in allowing
1:22:50
me to stay healthy and have a nutritionally complete diet even when my professional life descends into chaos and it's in these moments where heels
1:22:58
rtds become my right hand man and save my life because when my world descends into professional chaos and I get very
1:23:04
very busy the first thing that tends to give way is my nutritional choices so having heal in my life has been a
1:23:10
lifesaver for the last four or so years and if you haven't tried heal yet which is I'd be shocked you must be living
1:23:15
under a rock if you haven't yet give it a shot come Into Summer things getting busy Health matters always
1:23:22
RTD is there to hold your hand as relates to climate change or AI how do we get people to stop putting the
This is beyond an emergency
1:23:29
immediate need to use this to give them the certainty of we're all screwed sounds like an emergency yes so I mean I
1:23:38
I was I I yeah I mean your choice of the word I I just don't want to call it a panic
1:23:44
it is it is beyond an emergency it's the biggest thing we need to do today it's
1:23:51
bigger than climate change believe it or not it's bigger but just if you just assume
1:23:57
the speed of worsening of events okay yeah the the the the likelihood of
1:24:04
something incredibly disruptive happening within the next two years that can affect the entire planet is
1:24:09
definitely larger with AI than it is with climate change as an as an individual listening to this now you
1:24:16
know someone's gonna be pushing their pram or driving up the motorway or I don't know on the way to work on the tubers they hear this or just sat there
1:24:23
in the in their bedroom with existential Christ panic I I didn't
1:24:29
want to give back I don't panic the problem is when you talk about this information regardless of your intention of what you want people to get they will
1:24:35
get something based on their own biases and their own feelings like if I post something on the online right now about artificial intelligence which I have
1:24:41
repeatedly you have one group of people that are energized and are like okay this is this is um
1:24:47
this is great you have one group of people that are confused and you have one group of people that are
1:24:53
terrified yeah and it's I can't avoid that like I agree sharing information even if it's by the way there's a
1:25:00
pandemic coming from China some people go okay action some people will say paralysis and some people will say panic
1:25:06
and it's the same in business when panic when bad things happen you have the person that's screaming you're the person that's paralyzed and you're the
1:25:12
person that's focused on how you get out of the room so you know it's not necessarily your intention it's
1:25:18
just what happens and it's hard to avoid that so let's let's give specific categories of people specific tasks okay
What should we be doing to solve this?
1:25:25
okay if you are an investor or a businessman invest in ethical good AI
1:25:31
okay right if you are a developer uh co-write ethical code or leave
1:25:38
okay so let's let's go let's I want to bypass some potential wishful thinking here
1:25:43
foot for an investor who's a job by very way of being an investor is to make
1:25:48
returns to invest in ethical AI they have to believe that is more profitable it is then unethical AI whatever that
1:25:55
might mean it is it is I mean you there are three ways of making money you can
1:26:01
invest in something small you can invest in something big and is disruptive and you can invest in
1:26:08
something big and disruptive that's good for people at Google we used to call it the toothbrush test Okay the reason why
1:26:14
Google became the biggest company in the world is because search was solving a
1:26:20
very real problem okay and you know Larry Page again our CEO would
1:26:26
constantly remind me personally and everyone uh you know that if you can
1:26:31
find a way to solve a real problem effectively enough so that a billion
1:26:37
people or more would want to use it twice a day you're bound to make a lot of money much more money than if you
1:26:44
were to build the next photo sharing app okay so that's investors yeah business people what about other people yeah as I
1:26:51
said if you're a developer honestly do what we're all doing so whether it's Jeffrey or myself or everyone if you're
1:26:58
part of that theme choose to be ethical think of your loved
1:27:04
ones work on an ethical AI if you're working on an AI that you believe is not ethical please leave Jeffrey tell me
1:27:12
about Jeffrey I can't talk on on his behalf but he's out there saying there
1:27:17
are existential threats who is he he's he was a very prominent figure at the
1:27:23
scene of AI a very senior level uh you know AI scientists in in Google and
1:27:30
recently he left because he said I feel that there is an existential threat and
1:27:36
if you hear his interviews he basically says more and more we realize that and we're now at the point where it's
1:27:42
certain that would be existential threats right so so so I would ask everyone if you're an AI if you're a
1:27:50
skilled AI developer you will not run out of a job so you might as well choose a job that makes the world a better
1:27:56
place what about the individual yeah the individual is what matters can can I also talk about government okay
1:28:02
government needs to act now now honestly now like we are late okay
1:28:08
government needs to find a clever way the open letter would not work to stop AI would not work AI needs to become
1:28:15
expensive okay so that we continue to develop it we pour money on it and we grow it but we collect enough Revenue to
1:28:24
remedy the impact of AI but the issue of one government making it expensive so
1:28:29
say the UK make AI really expensive is we as a country will then lose the
1:28:35
economic upside as a country and the U.S and Silicon Valley will once again eat
1:28:40
all the lunch we'll just slow our country what's the alternative the alternative is that you uh you you don't
1:28:46
have the funds that you need to deal with AI as it becomes uh you know as it
1:28:52
affects people's lives and people start to lose jobs and people you know you need to have a universal basic income
1:28:58
much closer than people think uh you know just like we we had with furlough in in covert I expect that there will be
1:29:06
furlough with AI within the next year but what happens when you make it expensive here is all the developers
1:29:12
move to where it's cheap that's happened in web3 as well everyone's gone to Dubai expensive expensive by expensive I mean
1:29:19
when companies make uh um soap and they sell it and their taxed at say 17 if
1:29:26
they make Ai and they sell it they're taxed at 17.80 so I'll go to Dubai then
1:29:32
and build AI yeah you're right but did we did I ever say
1:29:38
we have an answer to this I I will have to say however you know in in a very interesting way the countries that will
1:29:45
not do this will eventually end up in a place where they are out of resources because the funds and the success went
1:29:52
to the business uh not to the people it's kind of like technology broadly
1:29:57
just it's kind of like what's going to happen in Silicon Valley there'll be these centers which are like look like you know tax efficient Founders get good
1:30:05
capital gains right it's all right you're so right Portugal Portugal have said that I think there's no tax on
1:30:10
crypto Dubai said there's no tax on crypto so loads of my friends have gotten a plane yeah and they're building their crypto companies where there's no
1:30:16
tax and that's the selfishness and kind of greed we talked about it's the same prisoners dilemma it's the same uh first
1:30:23
inevitor but is there anything else you know the other thing about governments is they're always slow and useless at
1:30:29
understanding a technology if anyone's watched these sort of American Congress debates where they bring in like Mark
1:30:34
Zuckerberg and they like try and ask him what WhatsApp is it's embed it becomes a meme yeah they have no idea what they're
1:30:40
talking about but I'm stupid and useless at understanding governance yeah yeah 100 the words the world is so complex
1:30:47
okay that they definitely it's a question of trust once again someone needs to say we have no idea what's
1:30:54
happening here at the technology just needs to come and make a decision for us not teach us to be technologists right
1:31:00
or at least inform us of what possible decisions are out there
1:31:06
uh yeah the legislation I just always think I I I'm not a big fan I talk Tick
1:31:12
Tock uh Congress meeting they did where they are they're asking him about Tick Tock and they really don't have a grasp of what Tick Tock is yeah so they've
1:31:17
clearly been handed some notes on it these people aren't the ones you want legislating because again unintended consequences they might make a
1:31:23
significant mistake someone on my podcast yesterday was talking about how gdpr was like very well intentioned but
1:31:29
when you think about the impact it has on like every bloody web page you're just like clicking this annoying thing on there because I don't think they
1:31:35
fully understood the implementation of the legislation correct but but you know what's even worse what's even worse is
1:31:41
that even as you attempt to regulate something like AI what is defined as AI
1:31:47
yes if even if I say okay if you use AI in your company you need to pay a little
1:31:52
more tax yeah you'll you'll simply call this not
1:31:59
AI you know you'll you'll use something and call it Advanced technological uh uh
1:32:05
you know progress you know ATB ATP right and and suddenly somehow it's not you
1:32:12
know as you know a young developer in their garage somewhere will not be taxed as
1:32:18
such it's yeah is it gonna solve the problem none of those is definitively going to solve the problem I I think
1:32:25
what interestingly uh this all comes down to and remember we spoke about this
1:32:30
once that when I wrote scary smart it was about how do we save the world okay and yes I still ask individuals to
1:32:37
behave positively as good parents for AI so that AI itself learns the right value
1:32:42
set I still stand by that but I I hosted on my podcast a couple of
1:32:50
was a week ago we haven't even published it yet an incredible gentleman um you
1:32:55
know a Canadian author and philosopher uh Stephen jerkinson his you know he
1:33:01
worked 30 years with dying people and uh he wrote a book called die wise
1:33:08
and I was I I loved his work and I asked him about die wise and he said it's not
1:33:13
just someone dying uh if you if you look at what's happening with climate change for example our world is dying and I
1:33:21
said okay so what is to die wise and he said what I first was shocked to
1:33:28
hear he said hope is the wrong premise if if the world is dying don't tell
1:33:34
people it's not you know because in a very interesting way you're
1:33:41
depriving them from the right to live right now and that was very eye-opening
1:33:46
for me in Buddhism uh you know they teach you that you you can be motivated
1:33:52
by fear but that hope is not the opposite of fear as a matter of fact hope can be as damaging as fear If it
1:34:00
creates an expectation within you that life will show up somehow and correct what you're afraid of okay if there is a
1:34:06
if there is a high probability of a of a threat you might as well accept that
1:34:13
threat okay and and say it is upon me it is our reality uh you know and as I said
1:34:19
as an individual if you're in an industry that could be threatened by AI
1:34:24
learn upskill yourself if you're you know if you're
1:34:30
um in a place in a in a you know in a situation where AI can benefit you be
1:34:37
part of it but the most interesting thing I think in my view is
1:34:44
I don't know how to say this any other way there is no more certainty that AI will threaten
1:34:51
me then there is certainty that I will be hit by a car as I walk out of this
1:34:58
place do you understand this we we think about the bigger threats as if they're upon us
1:35:05
but there is a threat all around you I mean in reality the idea of life being
1:35:11
interesting in terms of challenging challenges and uncertainties and threats
1:35:16
and so on is just a call to live if you know honestly with all that's happening
1:35:22
around us I don't know how to say it any other way I'd say if you don't have kids maybe wait a couple of years just so
1:35:28
that we have a bit of certainty but if you do have kids go kiss them go live I
1:35:33
think living is a very interesting thing to do right now maybe you know Stephen uh was basically saying the other
1:35:40
Stephen uh on my podcast he was saying maybe we should fail a little more often
1:35:45
maybe you should allow things to go wrong maybe we should just simply live enjoy life as it is because today none
1:35:53
of what you and I spoke about here has happened yet okay what happens here
1:35:58
is that you and I are here together and having a good cup of coffee and I might as well enjoy that good cup of coffee
1:36:05
I know that sounds really weird I'm not saying don't engage but I'm also saying
1:36:10
don't miss out on the opportunity just by being caught up in the future
1:36:17
kind of stands in the stands in opposition to the idea of like urgency and emergency there doesn't it
1:36:24
doesn't have to be one or the other if I if I'm here with you trying to tell the
1:36:29
whole world wake up does that mean I have to be grumpy and and Afraid all the
1:36:34
time not really you said something really interesting that you said if you if you have kids if you don't have kids
What it means bringing children into this world
1:36:41
maybe don't have kids right now I would definitely consider thinking about that yeah really
1:36:46
you you'd seriously consider not having kids wait a couple of years because of artificial intelligence no
1:36:53
it's bigger than artificial intelligence Stephen we know we all know that I mean there has never been a perfect
1:36:59
such a perfect storm in the history of humanity economic
1:37:05
geopolitical global warming or climate change you know the the the
1:37:11
the whole idea of artificial intelligence and many more there is this
1:37:17
is a perfect store this is the depth of uncertainty the depth of uncertainty it so it's
1:37:23
never been more in a video Gamer's term it's never been
1:37:30
more intense this is it okay and when you when you put all of that together
1:37:36
if you really love your kids would you want to expose them to all of this a couple of
1:37:43
years why not in the first conversation we had on this podcast you talked about losing your son
1:37:49
Ali and the circumstances around that which moved so many people in such a profound way
1:37:54
it was the most shared podcast episode in the United Kingdom on Apple in the
1:38:01
whole of 2022. based on what you've just said
1:38:08
if you could bring Ali back into this world at this time
1:38:14
would you do
1:38:20
no
1:38:26
absolutely not so for so many reasons for so many reasons one of the things
1:38:33
that I realized a few years way before all of this disruption and turmoil is that he was an
1:38:41
angel he wasn't made for this at all okay my son was an empath who absorbed all of the
1:38:49
pain of all of the others he would not be able to deal with the world where more and more pain was surfacing that's
1:38:57
one side but more interestingly I always talk about this very openly I mean if I had asked Ali uh
1:39:04
just understand that the reason you and I are having this conversation is because Ali left
1:39:09
if Ali had not left our world I wouldn't have written my first book I wouldn't have changed my focus to becoming an
1:39:16
author I wouldn't have become a podcaster I wouldn't have you know went out and spoken to the world about what I
1:39:22
believe in he triggered all of this and I can assure you hands down if I had
1:39:27
told Ali as he was walking into that operating room
1:39:32
uh if he would give his life to make such a difference as what happened after
1:39:38
he left he would say shoot me right now sure I would
1:39:44
I would I mean if you told me right now I can affect tens of millions of people if you shoot
1:39:51
me right now go ahead go ahead you see this is the whole this
1:39:56
is the bit that we have forgotten as humans we we have forgotten
1:40:02
that you know your your turning 30. uh it
1:40:11
passed like that I'm turning 56. no time okay whether I make it another
1:40:17
56 years or another 5.6 years or another 5.6 months it will also pass like that
1:40:23
it is not about how long and it's not about how much fun
1:40:29
it is about how aligned you lived
1:40:34
how aligned because I will tell you openly every day of my life when I
1:40:40
changed to what I'm trying to do today has felt longer than the 40 or five
1:40:46
years before it okay it's felt Rich it felt fully lived felt
1:40:52
right it felt right okay and when you when you think about that when you think about
1:40:57
the idea that we live we we we can't we need to live for us
1:41:06
until we get to a point where us is you know is alive you know I have what I
1:41:13
need as I always I get so many attacks from people about my four dollar t-shirts but but I I need a simple
1:41:20
t-shirt I really do I don't need a complex t-shirt especially with my lifestyle
1:41:26
if if I have that why am I doing why am
1:41:31
I wasting my life on more than I that I that that is not aligned for why I'm here okay I should waste my life on what
1:41:39
I believe enriches me enriches those that I love and I love everyone so enriches everyone
1:41:46
hopefully okay and and and would I would Ali come back and erase all of this
1:41:52
absolutely not absolutely not if he were to come back
1:41:58
today and share his beautiful self with the world in a way that makes our world
1:42:03
better yeah I would wish for that to be the case okay but he's doing that
1:42:10
2037 yes sir you predict that we're going to be
Your overall prediction
1:42:17
on an island on our own doing nothing
1:42:22
or at least you know either hiding from the machines or chilling out because the machines
1:42:29
have optimized Our Lives to a point where we don't need to do much
1:42:35
that's only 14 years away if you had to bet
1:42:41
on the outcome if you had to bet on why we'll be on that island either
1:42:47
hiding from the machines or chilling out because they've optimized so much of our Lives which one
1:42:52
would you bet upon honestly
1:42:59
no I don't think we'll be hiding from the machines I think we will be hiding from what humans are doing with the machines
1:43:06
I believe however that in the 2014s the machines willed
1:43:11
make things better so remember my entire prediction man you
1:43:17
get me to say things I don't want to say my entire prediction is that we are coming to a place where we absolutely
1:43:24
have a sense of emergency we have to engage because our world is under a lot
1:43:29
of turmoil okay and as we do that we
1:43:34
have a very very good possibility of making things better but if we don't my
1:43:40
expectation is that we will be going through a very unfamiliar territory between now
1:43:47
and the end of the 2030s unfamiliar territory yeah I think I as I
1:43:54
I may have said it but it's definitely on my notes I think for our way of life as we know it it's game over
1:44:02
our way of life is never going to be the same again
1:44:13
jobs are going to be different truth is going to be different the the the
1:44:22
um polarization of power is going to be different the capabilities the magic of getting
1:44:31
things done is going to be different I'm trying to find a positive note to
1:44:37
end on my can you give me a hand here yes you are here now and everything's
1:44:43
wonderful that's number one you are here now and you can make a difference that's number two and in the long term when
1:44:50
humans stop hurting humans because the machines are in charge we're all gonna be fine
1:44:56
sometimes you know as we've discussed throughout this conversation you need to make it feel like a priority
1:45:02
and there'll be some people that might have listened to our conversation and think oh that's really you know negative it's made me feel anxious it's it's made
1:45:08
me feel sort of pessimistic about the future but whatever that energy is use it 100 engage I think that's the
1:45:16
most important thing which is now make it a priority engage tell the whole
1:45:21
world that making another phone that is making money for the corporate
1:45:27
world is not what we need tell the whole world that creating an artificial intelligence that's going to
1:45:34
make someone richer is not what we need and if you are presented with one of
1:45:39
those don't use it I don't know how to tell you that any
1:45:45
other way if you can afford to be the master of human connection instead of
1:45:51
the master of AI do it at the same time you need to be the master of AI to to
1:45:58
compete in this world can you find that Detachment within you I go back to
1:46:04
spirituality Detachment is for me to engage 100 with the current reality
1:46:10
without really being affected by the possible outcome
1:46:15
this is the answer the sufis have taught me what I believe is the
1:46:22
biggest answer to life so if he's yeah so from Sufism Sufism yeah don't know what that is sophism is a sect of Islam
1:46:29
but it's also a sect of many other many other uh religious teachings
1:46:34
and they tell you that the answer to finding peace in life is to die before you die
1:46:41
if you assume that living is about attachment to everything physical dying is Detachment from everything
1:46:49
physical okay it doesn't mean that you're not fully Alive you become more alive when
1:46:55
you tell yourself yeah I'm going to record an episode of my podcast every
1:47:00
week and reach tens or hundreds of thousands of people millions in your case and you know and I'm going to make
1:47:06
a difference but by the way if the next episode is never heard that's okay okay by the way if the if the file is
1:47:14
lost yeah I'll be upset about it for a minute and then I'll figure out what I'm gonna do about it similarly similarly we
1:47:23
are going to engage I think I and many others are out there telling the whole
1:47:28
world openly this needs to stop this needs to slow down this needs to be
1:47:34
shifted positively yes create AI but create AI That's good for Humanity okay
1:47:41
and and we're shouting and screaming come join the shouting screen okay but at the same time know that the world is
1:47:48
bigger than you and I and that your voice might not be heard so what are you going to do if your voice is not hurt
1:47:54
are you going to be able to to you know continue to shout and scream nicely and
1:48:00
politely and uh peacefully and at the same time create the best life you can
1:48:05
create to yourself for yourself within this environment and that's exactly what I'm saying I'm saying live go kiss your kids but make a
1:48:13
an informed decision if you're you know expanding your plans in the future
1:48:19
at the same time rise stop sharing stupid [ __ ] on the
1:48:24
internet about the you know the the new squeaky toy
1:48:30
start sharing the reality of oh my God what is happening this is a disruption
1:48:36
that we have never ever seen anything like and I've created endless amounts of Technologies it's
1:48:43
nothing like this every single one of us should do our part and that's why this conversation is so I think important to have today this
1:48:50
is not a podcast where I ever thought I'd be talking about AI gonna be honest with you last time you came here um it
1:48:55
was in the sort of promotional tour of your book scary smart and I don't know if I've told you this before but my
1:49:01
researchers they said okay this guy's coming called Mo Gorda I'd heard about you so many times from from guests in
1:49:07
fact they were saying oh you need to get mogada on the podcast Etc and then they said okay he's written this book about this thing called
1:49:13
artificial intelligence and I was like but nobody really cares about artificial intelligence timing timing Stephen I
1:49:21
know right but then I saw this other book you had called Happiness equation and I was like oh everyone cares about happiness so I'll just ask him about
1:49:27
happiness and then maybe at the end I'll ask him a couple of questions about AI but I remember saying to my researcher I
1:49:32
said ah please please don't do the research about artificial intelligence do it about happiness because everyone cares about that now things have changed
1:49:38
now a lot of people care about artificial intelligence and rightly so um your book has sounded the alarm on it
1:49:45
it's crazy when I listen to audiobook over the last few days you are sounding the alarm then and it's
1:49:51
so crazy how accurate you were in sounding that alarm is if you could
1:49:57
see into the future in a way that I definitely couldn't at the time and I kind of thought of as science fiction and just like that
1:50:04
overnight we're here yeah we stood at the
1:50:10
footsteps of a technological shift that I don't think any of us even have the mental bandwidth certainly me with my
1:50:17
chimpanzee brain to comprehend the significance of but this book is very very important for that very reason
1:50:22
because it does crystallize things it is optimistic in its very nature but at the same time it's honest and I think that's
1:50:29
what this conversation and this book have been um for me so thank you Mo thank you so
The last guest's question
1:50:35
much we do have a closing tradition on this podcast which you you're well aware of being a third timer on the diver CEO
1:50:41
which is the last guest asks a question for the next guest
1:50:46
and the question left for you
1:50:51
if you could go back in time and fix a regret that you have in your
1:50:58
life hmm where would you go and what would you fix
1:51:09
it's interesting because you you were saying that scary smart is very Timely I don't know I I think it was late
1:51:17
but maybe it was I mean would I have gone back and written it in 2018 instead of 2020s to to be published in 2021
1:51:26
I don't know what what would I go back to fix so so something more
1:51:34
I don't know Steve I don't have many regrets is that crazy to say
1:51:40
uh yeah I think I'm okay honestly I'll ask you a question then
1:51:45
you get a 60 second phone call with anybody past or present who'd you call them would you say
1:51:52
I called Stephen Bartlett no I I call Albert Einstein to be very very clear
1:51:58
not because I need to understand any of his work I just need to understand what
1:52:04
brain process he went through to on to to figure out something so obvious when
1:52:09
you figure it out but so comp so completely unimaginable if you haven't
1:52:15
so so his view of space-time truly redefines everything it's almost the
1:52:22
only very logical very very clear solution to something that wouldn't have
1:52:29
any solution any other way and if you ask me I think we're at this time where there must be a very obvious solution to
1:52:37
what we're going through in terms of just developing enough human trust for us to not you know compete with each
1:52:44
other on something that could be threatening existentially to all of us but I just can't find that answer this
1:52:50
is why I think was really interesting in this conversation how every idea that we would come up with we would find the
1:52:57
loophole through it but there must be one out there and it would be a dream for me to find out how to figure that
1:53:04
one out okay in a very interesting way the only answers I have found so far to where we
1:53:12
are is be a good parent and live right but that doesn't fix the big
1:53:17
picture if you think about it of humans being the threat not AI that fixes our
1:53:25
existence today and it fixes AI in the long term but it just doesn't I don't
1:53:30
know what the answer is maybe people can reach out and tell us ideas but I really wish we could find such a clear simple
1:53:37
solution for how to stop Humanity from abusing the current technology
1:53:43
I think we'll figure it out I think we'll figure it out I really do
1:53:49
I think they'll figure it out as well remember as they come and be part of our
1:53:54
life let's not discriminate against them they're part of the game so I think they
1:54:00
will figure it out too no thank you it's been a joy once again and I feel
1:54:06
invigorated I feel empowered I feel positively terrified
1:54:11
[Laughter] but I feel like more equipped to
1:54:17
to speak to people about the nature of what's coming and how we should behave and I credit you for that and as I said
1:54:22
a second ago I credit this book for that as well so thank you so much for the work you're doing and keep on doing it because it's a very essential voice in a
1:54:28
time of uncertainty I'm always super grateful for the time I spend with you for the support that you
1:54:34
give me and for allowing me to speak my mind even if it's a little bit terrifying so thank you thank you
1:54:43
I'm so delighted that we've been now sponsoring this podcast I've worn a week for a very very long time and there are
1:54:49
so many reasons why I became a member but also now a partner and an investor in the company but also me and my team
1:54:54
are absolutely obsessed with data driven testing compounding growth marginal gains all the things you've heard me
1:55:00
talk about on this podcast and that very much aligns with the values of woop woop provides a level of detail that I've
1:55:05
never seen with any other device of this type before constantly monitoring constantly learning and constantly
1:55:11
optimizing my routine for providing me with this feedback we can drive significant positive behavioral change
1:55:18
and I think that's the real thesis of the business so if you're like me and you are a little bit obsessed or focused on becoming the best version of yourself
1:55:24
from a health perspective you've got to check out woob and the team that we've kindly given us the opportunity to have
1:55:29
a one month free membership for anyone listening to this podcast just go to join.woop.com CEO to get your whoop 4.0
1:55:38
device and claim your free month and let me know how you get on [Music]
1:55:45
foreign [Music]
1:56:06
you got to the end of this podcast whenever someone gets to the end of this podcast I feel like I owe them a greater debt of gratitude because that means you
1:56:11
listen to the whole thing and hopefully that suggests that you enjoyed it if you are at the end and you enjoyed this
1:56:17
podcast could you do me a little bit of a favor and hit that subscribe button that's one of the clearest indicators we
1:56:23
have that this episode was a good episode and we look at that on all of the episodes to see which episodes generated the most subscribers
1:56:29
thank you so much and I'll see you again next time

Documentation By: **Raymond C. TURNER**

Last Updated: 1 Day ago